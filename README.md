# ğŸ¤Ÿ Real-Time Sign Language Recognition System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.9.0-green.svg)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10.11-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**An advanced computer vision system for real-time American Sign Language (ASL) recognition using hand landmark detection and machine learning.**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [How It Works](#-how-it-works) â€¢ [Project Structure](#-project-structure)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Demo](#-demo)
- [Installation](#-installation)
- [Usage](#-usage)
- [How It Works](#-how-it-works)
- [Project Structure](#-project-structure)
- [Model Performance](#-model-performance)
- [Troubleshooting](#-troubleshooting)
- [Future Enhancements](#-future-enhancements)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸŒŸ Overview

This project implements a **real-time sign language recognition system** that can identify **36 different hand signs**:
- ğŸ”¤ **26 letters** (A-Z) of the American Sign Language alphabet
- ğŸ”¢ **10 digits** (0-9)

The system uses **MediaPipe** for hand landmark detection and a **Random Forest classifier** trained on hand gesture features to recognize signs with high accuracy.

---

## âœ¨ Features

- ğŸ¥ **Real-time Recognition**: Instant sign language detection through webcam
- ğŸ¤– **Machine Learning**: Random Forest classifier with 200 estimators for robust predictions
- ğŸ‘‹ **Hand Tracking**: 21-point hand landmark detection using Google's MediaPipe
- ğŸ“Š **Large Dataset**: Supports 200+ images per class for improved accuracy
- ğŸ¯ **High Accuracy**: Optimized hyperparameters for maximum performance
- ğŸš€ **Easy to Use**: Simple pipeline from data collection to inference
- ğŸ“¦ **Modular Design**: Clean separation of data collection, training, and inference

---

## ğŸ¬ Demo

### Recognition in Action
The system displays:
- âœ… Real-time hand landmark tracking
- ğŸ”² Bounding box around detected hand
- ğŸ”¤ Predicted letter/digit above the hand

### Supported Signs
- **Letters**: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z
- **Digits**: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- Webcam
- Linux/macOS/Windows

### Step 1: Clone the Repository
```bash
git clone <your-repository-url>
cd project2
```

### Step 2: Install Dependencies
```bash
pip install -r requirements.txt
```

### Dependencies
- `numpy==1.26.4` - Numerical computing
- `mediapipe==0.10.11` - Hand landmark detection
- `opencv-python==4.9.0.80` - Computer vision operations
- `scikit-learn==1.4.2` - Machine learning algorithms
- `pillow==10.2.0` - Image processing for GUI
- `pyttsx3==2.90` - Text-to-speech conversion

---

## ğŸ“– Usage

The system consists of **4 main scripts** that form a complete pipeline:

### 1ï¸âƒ£ Collect Training Images

Collect 200 images for each of the 36 sign classes:

```bash
python3 p_collect_images.py
```

**Instructions**:
- Position your hand to form the sign (A, B, C, etc.)
- Press **'Q'** when ready
- Hold the sign steady while 200 images are captured
- Repeat for all 36 signs

**Tips**:
- ğŸ’¡ Ensure good lighting
- ğŸ–ï¸ Keep hand centered in frame
- ğŸ”„ Vary hand position slightly for diversity
- â±ï¸ Each class takes ~10 seconds to capture

---

### 2ï¸âƒ£ Create Feature Dataset

Extract hand landmarks from collected images:

```bash
python3 p_create_dataset.py
```

**What it does**:
- Processes all images in `./data/` directory
- Detects hand landmarks using MediaPipe
- Normalizes coordinates relative to hand position
- Saves features to `data.pickle`

**Output**: Creates `data.pickle` containing normalized hand landmark features

---

### 3ï¸âƒ£ Train the Classifier

Train the Random Forest model on extracted features:

```bash
python3 p_train_classifier.py
```

**Training Configuration**:
- Algorithm: Random Forest Classifier
- Number of trees: 200
- Max depth: 20
- Train/Test split: 80/20
- Cross-validation: Stratified sampling

**Output**: 
- Displays accuracy score (e.g., "95.5% of samples were classified correctly!")
- Saves trained model to `model.p`

---

### 4ï¸âƒ£ Run Real-Time Recognition

Start the real-time sign language recognition:

```bash
python3 p_inference_classifier.py
```

**Features**:
- ğŸ¥ Live webcam feed
- ğŸ‘‹ Real-time hand landmark visualization
- ğŸ”² Bounding box around detected hand
- ğŸ”¤ Predicted sign displayed above hand
- âŒ¨ï¸ Press any key to exit

---

### 5ï¸âƒ£ Run Sign Language to Speech Conversion (NEW!)

Start the complete sign-to-speech application with GUI:

```bash
python3 p_sign_to_speech.py
```

**Features**:
- ğŸ¥ Live webcam feed with hand tracking
- ğŸ–ï¸ Real-time hand landmark visualization
- ğŸ”¤ Character-by-character text building
- ğŸ“ Sentence construction from recognized signs
- ğŸ”Š Text-to-speech conversion
- ğŸ¨ Clean, user-friendly GUI interface
- âŒ¨ï¸ Manual controls: Space, Backspace, Clear
- ğŸ—£ï¸ Speak button for audio output

**Interface Components**:
- **Video Feed**: Shows live camera with hand detection
- **Hand Landmarks**: Visual representation of detected hand landmarks
- **Character Display**: Current recognized character
- **Sentence Builder**: Accumulated text from recognized signs
- **Control Buttons**:
  - `Clear`: Reset sentence
  - `Speak`: Convert text to speech
  - `Space`: Add space between words
  - `Backspace`: Remove last character

---

## ğŸ”¬ How It Works

### Architecture Overview

```
ğŸ“· Webcam Input
    â†“
ğŸ‘‹ MediaPipe Hand Detection (21 Landmarks)
    â†“
ğŸ“ Feature Extraction (42 Normalized Coordinates)
    â†“
ğŸ¤– Random Forest Classifier (200 Trees)
    â†“
ğŸ”¤ Predicted Sign Output
```

### Technical Details

#### 1. Hand Landmark Detection
- Uses **MediaPipe Hands** solution
- Detects **21 key points** on each hand:
  - Wrist
  - Thumb (4 points)
  - Index finger (4 points)
  - Middle finger (4 points)
  - Ring finger (4 points)
  - Pinky finger (4 points)

#### 2. Feature Engineering
- Extracts X and Y coordinates for all 21 landmarks
- **Normalization**: Subtracts minimum X and Y values
- Creates **42 features per sample** (21 points Ã— 2 coordinates)
- Makes the model **translation-invariant**

#### 3. Classification Model
- **Algorithm**: Random Forest Classifier
- **Hyperparameters**:
  - `n_estimators=200`: 200 decision trees
  - `max_depth=20`: Maximum tree depth
  - `random_state=42`: For reproducibility
  - `n_jobs=-1`: Parallel processing

#### 4. Class Mapping
```python
Classes 0-25  â†’ A-Z (chr(65+i))
Classes 26-35 â†’ 0-9 (str(i-26))
```

---

## ğŸ“ Project Structure

```
project2/
â”‚
â”œâ”€â”€ ğŸ“„ p_collect_images.py        # Step 1: Collect training images
â”œâ”€â”€ ğŸ“„ p_create_dataset.py        # Step 2: Extract hand landmarks
â”œâ”€â”€ ğŸ“„ p_train_classifier.py      # Step 3: Train ML model
â”œâ”€â”€ ğŸ“„ p_inference_classifier.py  # Step 4: Real-time recognition
â”œâ”€â”€ ğŸ“„ p_sign_to_speech.py        # Step 5: Sign-to-Speech GUI Application (NEW!)
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                   # Documentation (this file)
â”‚
â”œâ”€â”€ ğŸ“¦ model.p                     # Trained classifier (generated)
â”œâ”€â”€ ğŸ“¦ data.pickle                 # Feature dataset (generated)
â”‚
â””â”€â”€ ğŸ“ data/                       # Training images directory
    â”œâ”€â”€ 0/   # Class 0 (Letter A)
    â”œâ”€â”€ 1/   # Class 1 (Letter B)
    â”œâ”€â”€ 2/   # Class 2 (Letter C)
    â”œâ”€â”€ ...
    â”œâ”€â”€ 25/  # Class 25 (Letter Z)
    â”œâ”€â”€ 26/  # Class 26 (Digit 0)
    â”œâ”€â”€ ...
    â””â”€â”€ 35/  # Class 35 (Digit 9)
```

---

## ğŸ“Š Model Performance

### Expected Accuracy
- **Training Accuracy**: ~98-99%
- **Test Accuracy**: ~90-95%
- **Real-time Performance**: 30+ FPS

### Performance Factors
| Factor | Impact |
|--------|--------|
| ğŸ’¡ **Lighting** | High - Affects hand detection |
| ğŸ“· **Camera Quality** | Medium - Better resolution helps |
| ğŸ–ï¸ **Hand Position** | High - Centered hands work best |
| ğŸ¯ **Sign Precision** | High - Clear signs improve accuracy |
| ğŸ”„ **Training Data Variety** | Critical - More diverse = better |

### Optimization Tips
1. **Increase dataset_size** in `p_collect_images.py` (e.g., 300-500 images)
2. **Vary hand positions** during data collection
3. **Use consistent lighting** during training and inference
4. **Tune model hyperparameters** in `p_train_classifier.py`
5. **Adjust detection confidence** in MediaPipe settings

---

## ğŸ”§ Troubleshooting

### Common Issues

#### âŒ "ValueError: setting an array element with a sequence"
**Cause**: Inconsistent feature dimensions (multiple hands detected)

**Solution**: Ensure only one hand is visible during data collection

#### âŒ Camera not working
**Cause**: Permission issues or wrong camera index

**Solution**: 
```python
# In any script with cv2.VideoCapture(0)
cap = cv2.VideoCapture(1)  # Try different indices: 0, 1, 2
```

#### âŒ Low accuracy
**Cause**: Insufficient or poor-quality training data

**Solutions**:
- Collect more images per class (increase `dataset_size`)
- Ensure consistent hand positioning
- Improve lighting conditions
- Retrain with better data

#### âŒ "ModuleNotFoundError"
**Cause**: Missing dependencies

**Solution**:
```bash
pip install -r requirements.txt
```

#### âŒ Slow performance
**Cause**: CPU bottleneck

**Solutions**:
- Reduce video resolution
- Decrease MediaPipe detection confidence
- Use fewer trees in Random Forest (e.g., `n_estimators=100`)

---

## ğŸš€ Future Enhancements

### Planned Features
- [ ] ğŸ¥ Support for dynamic signs (motion-based)
- [ ] ğŸ¤ Two-handed sign recognition
- [ ] ğŸŒ Web interface for easy access
- [ ] ğŸ“± Mobile app deployment
- [ ] ğŸ§  Deep learning model (CNN/LSTM)
- [ ] ğŸ—£ï¸ Text-to-speech for recognized signs
- [ ] ğŸ“š Expanded sign language support (BSL, ISL, etc.)
- [ ] ğŸ“Š Real-time accuracy metrics display
- [ ] ğŸ’¾ Cloud-based model training
- [ ] ğŸ® Interactive learning mode

### Advanced Improvements
- **Data Augmentation**: Rotation, scaling, brightness variations
- **Deep Learning**: CNN-based feature extraction
- **Temporal Models**: LSTM for sign sequences
- **Transfer Learning**: Pre-trained hand pose models
- **Edge Deployment**: TensorFlow Lite for mobile/embedded systems

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. ğŸ´ Fork the repository
2. ğŸŒ± Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”ƒ Open a Pull Request

### Areas for Contribution
- ğŸ› Bug fixes
- ğŸ“ Documentation improvements
- âœ¨ New features
- ğŸ¨ UI/UX enhancements
- ğŸ§ª Test coverage
- ğŸŒ Internationalization

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com

---

## ğŸ™ Acknowledgments

- **Google MediaPipe** - For excellent hand tracking technology
- **OpenCV** - For computer vision tools
- **scikit-learn** - For machine learning algorithms
- **ASL Community** - For sign language resources and inspiration

---

## ğŸ“š References

- [MediaPipe Hands Documentation](https://google.github.io/mediapipe/solutions/hands.html)
- [American Sign Language Alphabet](https://www.nidcd.nih.gov/health/american-sign-language)
- [Random Forest Algorithm](https://scikit-learn.org/stable/modules/ensemble.html#forest)
- [OpenCV Documentation](https://docs.opencv.org/)

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­

---

<div align="center">

**Made with â¤ï¸ and Python**

[â¬† Back to Top](#-real-time-sign-language-recognition-system)

</div>
